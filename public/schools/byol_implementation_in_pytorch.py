# -*- coding: utf-8 -*-
"""BYOL_Implementation_in_Pytorch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R8YMCP0RbGvI8boSLCW9lT-nHjlToe8b

# BYOL-Pytorch  
Pytorch Implementation of BYOL: Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning (https://arxiv.org/abs/2006.07733).   
Major part of Code is inspired from https://github.com/sthalles/PyTorch-BYOL.  
The Code has more appropriate Naming Convention.
# Default Training
* Running the Python File without any changes trains BYOL with **CIFAR10** Dataset.
* All the Parameters are contained in ___Params Object___ in the script.
# Custom Training
* Change the __Dataset Object__.
* Update the Required Parameters in the ___Params Object___.

## Import Statement
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.data.dataloader
from torchvision import datasets
from torchvision.transforms import transforms
import torchvision.models
import cv2
import numpy as np
from tqdm import tqdm

np.random.seed(0)
torch.manual_seed(42)

"""# Augmentation Functions    
Given an image, augmentation is applied to create two different views. Augmentation used over here is very similar to that of [SimCLR](https://arxiv.org/abs/2002.05709).
### GaussianBlur(Class)   
![alt text](https://wikimedia.org/api/rest_v1/media/math/render/svg/dd16b16869269dba008d19c0969515a1d50b3ae2)
*   **Parameter** = Kernel Size
*   **Output**     = Gaussian Blur Transformed Image   
### Transforms(Function)
*   **Parameter** = Input Dimension of the Image.
*   **Output**     = Composes a torchvision.transforms Object with all the Transformation functions intact.
### MultiViewDataInjector(Class)
*   **Parameter** = Input Image.
*   **Output**    = Applies **Transforms** Function to result two different augmented image.


"""

class GaussianBlur(object):
    """blur a single image on CPU"""

    def __init__(self, kernel_size):
        radias = kernel_size // 2
        kernel_size = radias * 2 + 1
        self.blur_h = nn.Conv2d(3, 3, kernel_size=(kernel_size, 1),
                                stride=1, padding=0, bias=False, groups=3)
        self.blur_v = nn.Conv2d(3, 3, kernel_size=(1, kernel_size),
                                stride=1, padding=0, bias=False, groups=3)
        self.k = kernel_size
        self.r = radias

        self.blur = nn.Sequential(
            nn.ReflectionPad2d(radias),
            self.blur_h,
            self.blur_v
        )

        self.pil_to_tensor = transforms.ToTensor()
        self.tensor_to_pil = transforms.ToPILImage()

    def __call__(self, img):
        img = self.pil_to_tensor(img).unsqueeze(0)

        sigma = np.random.uniform(0.1, 2.0)
        x = np.arange(-self.r, self.r + 1)
        x = np.exp(-np.power(x, 2) / (2 * sigma * sigma))
        x = x / x.sum()
        x = torch.from_numpy(x).view(1, -1).repeat(3, 1)

        self.blur_h.weight.data.copy_(x.view(3, 1, self.k, 1))
        self.blur_v.weight.data.copy_(x.view(3, 1, 1, self.k))

        with torch.no_grad():
            img = self.blur(img)
            img = img.squeeze()

        img = self.tensor_to_pil(img)

        return img

def Transforms(Input_Dim,S=0.5):
    Color_Jitter = transforms.ColorJitter(0.8*S,0.8*S,0.8*S,0.2*S)
    Data_Transforms = transforms.Compose([transforms.RandomResizedCrop(size=Input_Dim[0]),
                                         transforms.RandomHorizontalFlip(),
                                         transforms.RandomApply([Color_Jitter],p=0.75),
                                         transforms.RandomGrayscale(p=0.2),
                                         GaussianBlur(int(0.1*Input_Dim[0])),
                                         transforms.ToTensor(),
                                        ])
    return Data_Transforms

class MultiViewDataInjector(object):
    def __init__(self,Transforms):
        self.transforms = Transforms
    def __call__(self,Sample,*Consistent_Flip):
        if Consistent_Flip:
            Sample  =  torchvision.transforms.RandomHorizontalFlip()
        Output = [transforms(Sample) for transforms in self.transforms]
        return Output

"""# Model   
Contains two basic Neural Networks
*  **MLP_Base** - Creates the **Latent Space** from the Encoder.
*  **Skeleton Net** - Encompases MLP_BASE for **Latent Space** creation and uses **ResNet18** to learn Feature Representations.

"""

class MLP_Base(nn.Module):
    def __init__(self,Inp,Hidden,Projection):
        super(MLP_Base,self).__init__()
        self.Linear1 = nn.Linear(Inp,Hidden)
        self.BatchNorm = nn.BatchNorm1d(Hidden)
        self.Linear2 = nn.Linear(Hidden,Projection)
    def forward(self,Input):
        Linear_Inp = torch.relu(self.BatchNorm(self.Linear1(Input)))
        Linear_Out = self.Linear2(Linear_Inp)
        return Linear_Out

class SkeletonNet(nn.Module):
    def __init__(self,Hid,Proj):
        super(SkeletonNet,self).__init__()
        Resnet = torchvision.models.resnet50(pretrained=True)
        self.Encoder = torch.nn.Sequential(*list(Resnet.children())[:-1])
        self.Proj = MLP_Base(Resnet.fc.in_features,Hid,Proj)
    def forward(self,Input):
        Enc_Out = self.Encoder(Input)
        Enc_Out = Enc_Out.view(Enc_Out.size(0),Enc_Out.size(1))
        Final = self.Proj(Enc_Out)
        return Final

"""# Training Class"""

class BYOL:
    def __init__(self,Online_Net,Target_Net,Predictor,Optim,Params):
        self.Online_Net = Online_Net
        self.Target_Net = Target_Net
        self.Predictor  = Predictor
        self.Optim      = Optim
        self.Device     = Params['Device']
        self.Epochs     = Params['Epochs']
        self.Moment        = Params['M']
        self.Batch_Size = Params['Batch_Size']
        self.Save_Path = 'Models/BYOL.pth'
    @torch.no_grad()
    def Update_Target_Params(self):
        for Param_Online,Param_Target in zip(self.Online_Net.parameters(),self.Target_Net.parameters()):
            Param_Target = Param_Target.data *self.Moment + Param_Online.data*(1-self.Moment)
    @staticmethod
    def Loss(Rep1,Rep2):
        Norm_Rep1 = F.normalize(Rep1,dim=-1,p=2) #L2-Normalized Rep One
        Norm_Rep2 = F.normalize(Rep2,dim=-1,p=2) #L2 Normalized Rep Two
        Loss = -2 * (Norm_Rep1*Norm_Rep2).sum(dim=-1)
        return Loss
    def Init_Target_Network(self):
        for Param_Online,Param_Target in zip(self.Online_Net.parameters(),self.Target_Net.parameters()):
            Param_Target.data.copy_(Param_Online.data) #Init Target with Param_Online
            Param_Target.requires_grad = False
    def TrainLoop(self,View1,View2):
        Pred1 = self.Predictor(self.Online_Net(View1))
        Pred2 = self.Predictor(self.Online_Net(View2))
        with torch.no_grad():
            Target2 = self.Target_Net(View1)
            Target1 = self.Target_Net(View2)
        Loss_Calc = self.Loss(Pred1,Target1) + self.Loss(Pred2,Target2)
        return Loss_Calc.mean()
    def Train(self,Trainset):
        TrainLoader = torch.utils.data.DataLoader(Trainset,batch_size=self.Batch_Size,drop_last=False,shuffle=True)
        self.Init_Target_Network()
        for Epoch in range(self.Epochs):
          Loss_Count = 0.0
          print("Epoch {}".format(Epoch))
          for (View_1,View_2),_ in tqdm(TrainLoader):
              View_1 = View_1.to(self.Device)
              View_2 = View_2.to(self.Device)
              Loss = self.TrainLoop(View_1,View_2)
              Loss_Count += Loss.item()
              Loss.backward()
              self.Optim.step()
              self.Update_Target_Params()
          Epoch_Loss = Loss_Count/len(TrainLoader)
          print("\n Epoch {} Loss:{} : ".format(Epoch,Epoch_Loss))
        os.makedirs(os.path.dirname(self.Save_Path), exist_ok=True)
        self.Save(self.Save_Path)
    def Save(self,Save):
        torch.save({'Online_Net':self.Online_Net.state_dict(),
                    'Enc_Net':self.Online_Net.Encoder.state_dict(),
                    'Target_Net':self.Target_Net.state_dict(),
                    'Optim':self.Optim.state_dict()},Save)

import matplotlib.pyplot as plt

class BYOL:
    def __init__(self,Online_Net,Target_Net,Predictor,Optim,Params):
        self.Online_Net = Online_Net
        self.Target_Net = Target_Net
        self.Predictor  = Predictor
        self.Optim      = Optim
        self.Device     = Params['Device']
        self.Epochs     = Params['Epochs']
        self.Moment        = Params['M']
        self.Batch_Size = Params['Batch_Size']
        self.Save_Path = 'BYOL.pth'
    @torch.no_grad()
    def Update_Target_Params(self):
        for Param_Online,Param_Target in zip(self.Online_Net.parameters(),self.Target_Net.parameters()):
            Param_Target = Param_Target.data *self.Moment + Param_Online.data*(1-self.Moment)
    @staticmethod
    def Loss(Rep1,Rep2):
        Norm_Rep1 = F.normalize(Rep1,dim=-1,p=2) #L2-Normalized Rep One
        Norm_Rep2 = F.normalize(Rep2,dim=-1,p=2) #L2 Normalized Rep Two
        Loss = -2 * (Norm_Rep1*Norm_Rep2).sum(dim=-1)
        return Loss
    def Init_Target_Network(self):
        for Param_Online,Param_Target in zip(self.Online_Net.parameters(),self.Target_Net.parameters()):
            Param_Target.data.copy_(Param_Online.data) #Init Target with Param_Online
            Param_Target.requires_grad = False
    def TrainLoop(self,View1,View2):
        self.Optim.zero_grad()
        Pred1 = self.Predictor(self.Online_Net(View1))
        Pred2 = self.Predictor(self.Online_Net(View2))
        with torch.no_grad():
            Target2 = self.Target_Net(View1)
            Target1 = self.Target_Net(View2)
        Loss_Calc = self.Loss(Pred1,Target1) + self.Loss(Pred2,Target2)
        return Loss_Calc.mean()


    def Train(self, Trainset):
        """Modified Train method to include accuracy tracking."""
        TrainLoader = torch.utils.data.DataLoader(Trainset, batch_size=self.Batch_Size, drop_last=False, shuffle=True)
        self.Init_Target_Network()

        epoch_accuracies = []
        for Epoch in range(self.Epochs):
            Loss_Count = 0.0
            print("Epoch {}".format(Epoch))
            for (View_1, View_2), _ in tqdm(TrainLoader):
                View_1 = View_1.to(self.Device)
                View_2 = View_2.to(self.Device)
                Loss = self.TrainLoop(View_1, View_2)
                Loss_Count += Loss.item()
                self.Optim.zero_grad()
                Loss.backward()
                self.Optim.step()
                self.Update_Target_Params()

            Epoch_Loss = Loss_Count / len(TrainLoader)
            print(f"Epoch {Epoch + 1} Loss: {Epoch_Loss:.4f}")


        self.Save(self.Save_Path)




    def Save(self,Save):
        torch.save({'Online_Net':self.Online_Net.state_dict(),
                    'Enc_Net':self.Online_Net.Encoder.state_dict(),
                    'Target_Net':self.Target_Net.state_dict(),
                    'Optim':self.Optim.state_dict()},Save)

"""# Main Training"""

from torchvision.datasets import ImageFolder

domain = 'clipart'
domain_path = f'/content/drive/MyDrive/source'

Parameters = {
    'Epochs': 100,
    'M': 0.99,
    'Batch_Size': 128,
    'Device': 'cuda',
    'Hidden': 1024,
    'Proj': 256,
    'LR': 0.03
}

Data_Transforms = Transforms((224, 224))
Dataset = ImageFolder(domain_path, transform=MultiViewDataInjector([Data_Transforms, Data_Transforms]))

Online_Network = SkeletonNet(Parameters['Hidden'], Parameters['Proj'])
Predictor = MLP_Base(Online_Network.Proj.Linear2.out_features, Parameters['Hidden'], Parameters['Proj'])
Target_Network = SkeletonNet(Parameters['Hidden'], Parameters['Proj'])

from google.colab import drive
drive.mount('/content/drive')

Online_Network.to(Parameters['Device'])
Predictor.to(Parameters['Device'])
Target_Network.to(Parameters['Device'])
print("Models Made.")

Optimizer = torch.optim.SGD(list(Online_Network.parameters())+list(Predictor.parameters()),lr=0.01)

Trainer = BYOL(Online_Network,Target_Network,Predictor,Optimizer,Parameters)

Trainer.Train(Dataset)

Trainer.Save(f'/content/drive/MyDrive/BYOL.pth')

# BYOL Model Evaluation Script
# Add this as a new cell in your notebook

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import transforms, datasets
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns

# Evaluation transforms (simpler than training transforms)
def get_eval_transforms():
    return transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

# Linear evaluation function
def linear_evaluation(encoder, train_loader, test_loader, device, num_classes, epochs=20):
    """
    Perform linear evaluation on the learned representations
    """
    # Create linear classifier
    classifier = nn.Linear(2048, num_classes).to(device)  # ResNet50 encoder output is 2048
    nn.init.xavier_uniform_(classifier.weight)

    optimizer = torch.optim.SGD(classifier.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)
    criterion = nn.CrossEntropyLoss()
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)

    # Freeze encoder
    encoder.eval()
    for param in encoder.parameters():
        param.requires_grad = False

    best_accuracy = 0.0
    train_accuracies = []
    test_accuracies = []

    print("Starting Linear Evaluation...")
    print("-" * 50)

    for epoch in range(epochs):
        # Training phase
        classifier.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0

        for images, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} - Training"):
            images, labels = images.to(device), labels.to(device)

            # Extract features
            with torch.no_grad():
                features = encoder(images)
                features = features.view(features.size(0), -1)  # Flatten

            # Forward pass
            outputs = classifier(features)
            loss = criterion(outputs, labels)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # Statistics
            train_loss += loss.item()
            _, predicted = outputs.max(1)
            train_correct += predicted.eq(labels).sum().item()
            train_total += labels.size(0)

        train_accuracy = 100.0 * train_correct / train_total
        train_accuracies.append(train_accuracy)

        # Validation phase
        classifier.eval()
        test_loss = 0.0
        test_correct = 0
        test_total = 0
        all_predictions = []
        all_labels = []

        with torch.no_grad():
            for images, labels in tqdm(test_loader, desc=f"Epoch {epoch+1}/{epochs} - Testing"):
                images, labels = images.to(device), labels.to(device)

                # Extract features
                features = encoder(images)
                features = features.view(features.size(0), -1)

                # Forward pass
                outputs = classifier(features)
                loss = criterion(outputs, labels)

                # Statistics
                test_loss += loss.item()
                _, predicted = outputs.max(1)
                test_correct += predicted.eq(labels).sum().item()
                test_total += labels.size(0)

                all_predictions.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())

        test_accuracy = 100.0 * test_correct / test_total
        test_accuracies.append(test_accuracy)

        scheduler.step()

        if test_accuracy > best_accuracy:
            best_accuracy = test_accuracy

        print(f"Epoch [{epoch+1}/{epochs}]")
        print(f"Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_accuracy:.2f}%")
        print(f"Test Loss: {test_loss/len(test_loader):.4f}, Test Acc: {test_accuracy:.2f}%")
        print(f"Best Test Acc: {best_accuracy:.2f}%")
        print("-" * 50)

    return best_accuracy, train_accuracies, test_accuracies, all_predictions, all_labels

# Feature extraction function
def extract_features(encoder, data_loader, device):
    """
    Extract features from the encoder for further analysis
    """
    encoder.eval()
    features_list = []
    labels_list = []

    with torch.no_grad():
        for images, labels in tqdm(data_loader, desc="Extracting features"):
            images = images.to(device)
            features = encoder(images)
            features = features.view(features.size(0), -1)

            features_list.append(features.cpu().numpy())
            labels_list.append(labels.numpy())

    features = np.concatenate(features_list, axis=0)
    labels = np.concatenate(labels_list, axis=0)

    return features, labels

# Visualization functions
def plot_training_curves(train_accuracies, test_accuracies):
    """
    Plot training and testing accuracy curves
    """
    plt.figure(figsize=(10, 6))
    epochs = range(1, len(train_accuracies) + 1)

    plt.plot(epochs, train_accuracies, 'b-', label='Training Accuracy')
    plt.plot(epochs, test_accuracies, 'r-', label='Testing Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy (%)')
    plt.title('Linear Evaluation - Training vs Testing Accuracy')
    plt.legend()
    plt.grid(True)
    plt.show()

def plot_confusion_matrix(y_true, y_pred, class_names=None):
    """
    Plot confusion matrix
    """
    cm = confusion_matrix(y_true, y_pred)

    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.title('Confusion Matrix')
    plt.show()

# Main evaluation function
def evaluate_byol_model(model_path, test_data_path, device='cuda', batch_size=64):
    """
    Complete evaluation pipeline for BYOL model
    """
    print("Loading BYOL model...")

    # Load the saved model
    checkpoint = torch.load(model_path, map_location=device)

    # Recreate the network architecture
    online_net = SkeletonNet(Parameters['Hidden'], Parameters['Proj'])
    online_net.load_state_dict(checkpoint['Online_Net'])
    online_net.to(device)
    online_net.eval()

    print("Model loaded successfully!")

    # Prepare data
    eval_transform = get_eval_transforms()

    # Load test dataset
    test_dataset = datasets.ImageFolder(test_data_path, transform=eval_transform)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)

    # For Office-Home dataset, we need to split into train/test or use different domains
    # Here we'll use the same domain for demonstration, but you should use different domains
    train_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4)

    num_classes = len(test_dataset.classes)
    print(f"Number of classes: {num_classes}")
    print(f"Classes: {test_dataset.classes}")
    print(f"Test samples: {len(test_dataset)}")

    # Perform linear evaluation
    best_accuracy, train_accs, test_accs, predictions, true_labels = linear_evaluation(
        online_net.Encoder, train_loader, test_loader, device, num_classes
    )

    # Print detailed results
    print("\n" + "="*60)
    print("EVALUATION RESULTS")
    print("="*60)
    print(f"Best Test Accuracy: {best_accuracy:.2f}%")

    # Classification report
    print("\nClassification Report:")
    print(classification_report(true_labels, predictions, target_names=test_dataset.classes))

    # Plot results
    plot_training_curves(train_accs, test_accs)
    plot_confusion_matrix(true_labels, predictions, test_dataset.classes)

    # Extract and analyze features
    print("\nExtracting features for analysis...")
    features, labels = extract_features(online_net.Encoder, test_loader, device)

    print(f"Feature shape: {features.shape}")
    print(f"Feature statistics:")
    print(f"  Mean: {features.mean():.4f}")
    print(f"  Std: {features.std():.4f}")
    print(f"  Min: {features.min():.4f}")
    print(f"  Max: {features.max():.4f}")

    return {
        'best_accuracy': best_accuracy,
        'train_accuracies': train_accs,
        'test_accuracies': test_accs,
        'features': features,
        'labels': labels,
        'predictions': predictions,
        'true_labels': true_labels,
        'class_names': test_dataset.classes
    }

# Usage example - Add this as the final cell to run evaluation
"""

"""

# Run the evaluation
results = evaluate_byol_model(
    model_path='/content/drive/MyDrive/BYOL.pth',  # Path to your saved model
    test_data_path='/content/drive/MyDrive/source',  # Path to test domain (e.g., target domain for domain adaptation)
    device='cuda',
    batch_size=64
)

print(f"Final Test Accuracy: {results['best_accuracy']:.2f}%")

# Run the evaluation
results = evaluate_byol_model(
    model_path='/content/drive/MyDrive/BYOL.pth',  # Path to your saved model
    test_data_path='/content/drive/MyDrive/target1',  # Path to test domain (e.g., target domain for domain adaptation)
    device='cuda',
    batch_size=64
)

print(f"Final Test Accuracy: {results['best_accuracy']:.2f}%")

"""unsupervised DA

train cosda with unsupervised method

train teacher model in supervised model
"""